{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix\n",
    "\n",
    "| Metric | Data Type(s) | Mathematic Definition | ArcPy | Source\n",
    "| :---         |     :---:      |    :---  |  :---: | :---: |\n",
    "| `Confusion Matrix`   | Categorical  | `BIC = -2 * log(L) + k * log(n)` <br> where: <br>  `L` is the maximized value of the likelihood function of the model <br> `k` is the number of parameters in the model <br>`n` is the number of observations in the data set  | N/A | ChatGPT\n",
    "\n",
    "Similar To: Accuracy, Precision, Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How To: Python \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy\n",
    "from sklearn import metrics\n",
    "\n",
    "actual = numpy.random.binomial(insert sample values) # Hypothetical actual vs predicted values\n",
    "predicted = numpy.random.binomial(insert sample values)\n",
    "\n",
    "confusion_matrix = metrics.confusion_matrix(actual, predicted)\n",
    "\n",
    "cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = confusion_matrix, display_labels = [False, True])\n",
    "\n",
    "cm_display.plot()\n",
    "plt.show()   "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy\n",
    "\n",
    "\n",
    "| Metric | Data Type(s) | Mathematic Definition | ArcPy | Source\n",
    "| :---         |     :---:      |    :---  |  :---: |  :---: |\n",
    "| `Accuracy`   | Categorical  | `Accuracy = (TP + TN) / (TP + TN + FP + FN)` <br> where: <br> `TP` (True Positives) is the number of correctly predicted positive instances <br> `TN` (True Negatives) is the number of correctly predicted negative instances <br> `FP` (False Positives) is the number of incorrectly predicted positive instances <br> `FN` (False Negatives) is the number of incorrectly predicted negative instances  |Yes | ChatGPT\n",
    "\n",
    "Similar To: Precision, Recall, ROC, Confusion Matrix <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How To: ArcPy\n",
    "\n",
    "import arcpy\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How To: Python \n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Example predicted and actual values\n",
    "predicted = [0, 1, 1, 0, 1, 0, 0, 1, 1]\n",
    "actual = [0, 1, 1, 1, 1, 0, 0, 1, 0]\n",
    "\n",
    "# Calculate accuracy score\n",
    "accuracy = accuracy_score(actual, predicted)\n",
    "\n",
    "# Print accuracy score\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision\n",
    "\n",
    "\n",
    "| Metric | Data Type(s) | Mathematic Definition | ArcPy | Source\n",
    "| :---         |     :---:      |    :---  |  :---: |  :---: |\n",
    "| `Precision`   | Categorical  | `Precision = TP / (TP + FP)` <br> where: <br> `TP` (True Positives) is the number of correctly predicted positive instances <br> `FP` (False Positives) is the number of incorrectly predicted positive instances | Yes | ChatGPT\n",
    "\n",
    "Similar To: Recall, Accuracy, ROC, Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How To: ArcPy\n",
    "\n",
    "import arcpy\n",
    "from sklearn.metrics import precision_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How To: Python \n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Example predicted and actual values\n",
    "predicted = [0, 1, 1, 0, 1, 0, 0, 1, 1]\n",
    "actual = [0, 1, 1, 1, 1, 0, 0, 1, 0]\n",
    "\n",
    "# Calculate accuracy score\n",
    "accuracy = accuracy_score(actual, predicted)\n",
    "\n",
    "# Print accuracy score\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recall\n",
    "\n",
    "\n",
    "| Metric | Data Type(s) | Mathematic Definition | ArcPy | Source\n",
    "| :---         |     :---:      |    :---  |  :---: |  :---: |\n",
    "| `Recall`   | Categorical  | `Recall = TP / (TP + FN)` <br> where: <br> `TP` (True Positives) is the number of correctly predicted positive instances <br> `FN` (False Negatives) is the number of incorrectly predicted negative instances | Yes | ChatGPT\n",
    "\n",
    "Similar To: Precision, Accuracy, ROC, Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How To: ArcPy\n",
    " \n",
    "import arcpy from sklearn.metrics import recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How To: Python \n",
    "\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "# Example predicted and actual values\n",
    "predicted = [0, 1, 1, 0, 1, 0, 0, 1, 1]\n",
    "actual = [0, 1, 1, 1, 1, 0, 0, 1, 0]\n",
    "\n",
    "# Calculate recall score\n",
    "recall = recall_score(actual, predicted)\n",
    "\n",
    "# Print recall score\n",
    "print(\"Recall:\", recall)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayesian Information Criterion (BIC)\n",
    "\n",
    "| Metric | Data Type(s) | Mathematic Definition | ArcPy | Source\n",
    "| :---         |     :---:      |    :---  |  :---: |  :---: |\n",
    "| `Bayesian Information Criterion (BIC)`   | Quanitative  | `BIC = -2 * log(L) + k * log(n)` <br> where: <br>  `L` is the maximized value of the likelihood function of the model <br> `k` is the number of parameters in the model <br> `n` is the number of observations in the data set | N/A | ChatGPT\n",
    "\n",
    "Similar To: Akaike’s Information Criterion (AIC), Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How To: Python \n",
    "\n",
    "import arcpy\n",
    "import numpy as np\n",
    "\n",
    "# Load the true values and predicted values into numpy arrays\n",
    "true_values = arcpy.RasterToNumPyArray(\"true_values.tif\")\n",
    "predicted_values = arcpy.RasterToNumPyArray(\"predicted_values.tif\")\n",
    "\n",
    "# Calculate the residual sum of squares (RSS)\n",
    "rss = np.sum((true_values - predicted_values) ** 2)\n",
    "\n",
    "# Calculate the degrees of freedom (df)\n",
    "df = true_values.size - 2\n",
    "\n",
    "# Calculate the log-likelihood\n",
    "log_likelihood = -0.5 * (df * np.log(2 * np.pi) + df * np.log(rss / df) + df + 1)\n",
    "\n",
    "# Calculate the Bayesian Information Criterion (BIC)\n",
    "bic = -2 * log_likelihood + np.log(df) * 2\n",
    "\n",
    "# Print the BIC value\n",
    "print(\"BIC value: \", bic)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Akaike’s Information Criterion (AIC)\n",
    "\n",
    "\n",
    "| Metric | Data Type(s) | Mathematic Definition | ArcPy | Source\n",
    "| :---         |     :---:      |    :---  |  :---: |  :---: |\n",
    "| `Akaike’s Information Criterion (AIC)`   | Quanitative  | `AIC = -2 * log(L) + 2 * k` <br> where: <br> L is the maximized value of the likelihood function of the model <br> k is the number of parameters in the model  | N/A | ChatGPT\n",
    "\n",
    "Similar To: Bayesian Information Criterion (BIC), Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How To: Python \n",
    "\n",
    "import arcpy\n",
    "import numpy as np\n",
    "\n",
    "# Load the true values and predicted values into numpy arrays\n",
    "true_values = arcpy.RasterToNumPyArray(\"true_values.tif\")\n",
    "predicted_values = arcpy.RasterToNumPyArray(\"predicted_values.tif\")\n",
    "\n",
    "# Calculate the residual sum of squares (RSS)\n",
    "rss = np.sum((true_values - predicted_values) ** 2)\n",
    "\n",
    "# Calculate the degrees of freedom (df)\n",
    "df = true_values.size - 2\n",
    "\n",
    "# Calculate the log-likelihood\n",
    "log_likelihood = -0.5 * (df * np.log(2 * np.pi) + df * np.log(rss / df) + df + 1)\n",
    "\n",
    "# Calculate the Akaike Information Criterion (AIC)\n",
    "aic = -2 * log_likelihood + 2 * 2\n",
    "\n",
    "# Print the AIC value\n",
    "print(\"AIC value: \", aic)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Residual Standard Error\n",
    "| Metric | Data Type(s) | Mathematic Definition | ArcPy | Source\n",
    "| :---         |     :---:      |    :---  |  :---: |  :---: |\n",
    "| `Residual Standard Error`   | Quanitative  | `RSE = sqrt( SSE / (n - k - 1) )` <br> where: <br> `SSE` is the sum of squared errors or residuals <br> `n` is the total number of observations <br> `k` is the number of predictor variables | N/A | ChatGPT\n",
    "\n",
    "Similar To: R-Squared, Adjusted R-Squared, Mean-Squared Error, RMSE, Residual Standard Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How To: Python \n",
    "\n",
    "import arcpy\n",
    "import numpy as np\n",
    "\n",
    "# Load the true values and predicted values into numpy arrays\n",
    "true_values = arcpy.RasterToNumPyArray(\"true_values.tif\")\n",
    "predicted_values = arcpy.RasterToNumPyArray(\"predicted_values.tif\")\n",
    "\n",
    "# Calculate the residual sum of squares (RSS)\n",
    "rss = np.sum((true_values - predicted_values) ** 2)\n",
    "\n",
    "# Calculate the degrees of freedom (df)\n",
    "df = true_values.size - 2\n",
    "\n",
    "# Calculate the residual standard error (RSE)\n",
    "rse = np.sqrt(rss / df)\n",
    "\n",
    "# Print the RSE value\n",
    "print(\"RSE value: \", rse)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean Absolute Error\n",
    "| Metric | Data Type(s) | Mathematic Definition | ArcPy | Source\n",
    "| :--- |  :---: | :---  | :---: | :---: |\n",
    "| `Mean Absolute Error`  | Quanitative  | where: <br> `n` is the number of instances in the dataset <br> `yi` is the i-th actual value <br> `ŷi` is the i-th predicted value | N/A | ChatGPT\n",
    "\n",
    "`MAE = (1/n) * Σ|i=1 to n| |yi - ŷi|`\n",
    "\n",
    "Similar To: R-Squared, Adjusted R-Squared, Mean-Squared Error, RMSE, Residual Standard Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How To: Python \n",
    "\n",
    "import arcpy\n",
    "import numpy as np\n",
    "\n",
    "# Load the true values and predicted values into numpy arrays\n",
    "true_values = arcpy.RasterToNumPyArray(\"true_values.tif\")\n",
    "predicted_values = arcpy.RasterToNumPyArray(\"predicted_values.tif\")\n",
    "\n",
    "# Calculate the mean absolute error (MAE)\n",
    "mae = np.mean(np.abs(predicted_values - true_values))\n",
    "\n",
    "# Print the MAE value\n",
    "print(\"MAE value: \", mae)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### R-Squared\n",
    "\n",
    "\n",
    "| Metric | Data Type(s) | Mathematic Definition | ArcPy | Source\n",
    "| :---         |     :---:      |    :---  |  :---: |  :---: |\n",
    "| `R-Squared`  | Quanitative  | `R-Squared = 1 - (SSres / SStot)` <br> where: <br> `SSres` the sum of squared residuals or the sum of squared differences between the predicted values and the actual values <br> `SStot` is the total sum of squares or the sum of squared differences between the actual values and the mean of the dependent variable <br> | N/A | ChatGPT\n",
    "\n",
    "Similar To: Root Mean Squared Error (RMSE), Mean-Squared Error (MSE), Adjusted R-squared, Mean Absolute Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How To: Python \n",
    "\n",
    "import arcpy\n",
    "import numpy as np\n",
    "\n",
    "# Load the true values and predicted values into numpy arrays\n",
    "true_values = arcpy.RasterToNumPyArray(\"true_values.tif\")\n",
    "predicted_values = arcpy.RasterToNumPyArray(\"predicted_values.tif\")\n",
    "\n",
    "# Calculate the mean of the true values\n",
    "true_mean = np.mean(true_values)\n",
    "\n",
    "# Calculate the total sum of squares (TSS)\n",
    "tss = np.sum((true_values - true_mean) ** 2)\n",
    "\n",
    "# Calculate the residual sum of squares (RSS)\n",
    "rss = np.sum((true_values - predicted_values) ** 2)\n",
    "\n",
    "# Calculate the R-squared value\n",
    "r_squared = 1 - (rss / tss)\n",
    "\n",
    "# Print the R-squared value\n",
    "print(\"R-squared value: \", r_squared)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Root Mean Square Error\n",
    "\n",
    "\n",
    "| Metric | Data Type(s) | Mathematic Definition | ArcPy | Source\n",
    "| :---         |     :---:      |    :---  |  :---: |  :---: |\n",
    "| `Root Mean Square Error`  | Quanitative  | where: <br> `n` is the number of instances in the dataset <br> `yi` is the i-th actual value <br> `ŷi` is the i-th predicted value <br> | N/A | ChatGPT\n",
    "\n",
    "`RMSE = sqrt((1/n) * Σ|i=1 to n| (yi - ŷi)^2)`\n",
    "\n",
    "Similar To: R-Squared, Adjusted R-Squared, Mean-Squared Error, Mean Absolute Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How To: Python \n",
    "\n",
    "import arcpy\n",
    "import numpy as np\n",
    "\n",
    "# Load the true values and predicted values into numpy arrays\n",
    "true_values = arcpy.RasterToNumPyArray(\"true_values.tif\")\n",
    "predicted_values = arcpy.RasterToNumPyArray(\"predicted_values.tif\")\n",
    "\n",
    "# Calculate the residual sum of squares (RSS)\n",
    "rss = np.sum((true_values - predicted_values) ** 2)\n",
    "\n",
    "# Calculate the mean squared error (MSE)\n",
    "mse = rss / true_values.size\n",
    "\n",
    "# Calculate the root mean squared error (RMSE)\n",
    "rmse = np.sqrt(mse)\n",
    "\n",
    "# Print the RMSE value\n",
    "print(\"RMSE value: \", rmse)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adjusted R-Squared\n",
    "\n",
    "\n",
    "| Metric | Data Type(s) | Mathematic Definition | ArcPy | Source\n",
    "| :---         |     :---:      |    :---  |  :---: |  :---: |\n",
    "| `Adjusted R-Squared`  | Quanitative  | `Adjusted R-Squared = 1 - [(1 - R2) * (n - 1) / (n - k - 1)]` <br> where: <br> `R2` is the coefficient of determination, which is a measure of how well the regression line fits the data <br> `n` is the number of observations or data points in the sample <br> `k` is the number of independent variables (or predictors) in the model <br> | N/A | ChatGPT\n",
    "\n",
    "Similar To: Root Mean Squared Error (RMSE), Mean-Squared Error (MSE), R-squared, Mean-Absolute Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How To: Python - Adjusted R-Squared\n",
    "\n",
    "import arcpy\n",
    "import numpy as np\n",
    "\n",
    "# Load the true values, predicted values, and number of independent variables into numpy arrays\n",
    "true_values = arcpy.RasterToNumPyArray(\"true_values.tif\")\n",
    "predicted_values = arcpy.RasterToNumPyArray(\"predicted_values.tif\")\n",
    "num_independent_vars = 3\n",
    "\n",
    "# Calculate the mean of the true values\n",
    "true_mean = np.mean(true_values)\n",
    "\n",
    "# Calculate the total sum of squares (TSS)\n",
    "tss = np.sum((true_values - true_mean) ** 2)\n",
    "\n",
    "# Calculate the residual sum of squares (RSS)\n",
    "rss = np.sum((true_values - predicted_values) ** 2)\n",
    "\n",
    "# Calculate the degrees of freedom for the residuals\n",
    "df_resid = true_values.size - num_independent_vars - 1\n",
    "\n",
    "# Calculate the degrees of freedom for the model\n",
    "df_model = num_independent_vars\n",
    "\n",
    "# Calculate the adjusted R-squared value\n",
    "adj_r_squared = 1 - ((rss / df_resid) / (tss / df_model))\n",
    "\n",
    "# Print the adjusted R-squared value\n",
    "print(\"Adjusted R-squared value: \", adj_r_squared)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Receiver Operator Characteristic (ROC) Curve and Area Under the Curve\n",
    "\n",
    "\n",
    "| Metric | Data Type(s) | Mathematic Definition | ArcPy | Source\n",
    "| :---         |     :---:      |    :---  |  :---: |  :---: |\n",
    "| `Receiver Operator Characteristic (ROC) Curve and Area Under the Curve`  |  Qualitative or Quanitative   | ROC Curve plots the true positive rate (TPR) against the false positive rate (FPR), where: TPR = TP / (TP + FN) and FPR = FP / (FP + TN). The Area Under the Curve is the area under the ROC curve, and it indicates the likelihood that the model will rank a randomly chosen positive instance higher than a randomly chosen negative instance. | N/A | ChatGPT\n",
    "\n",
    "Similar To: Precision, Recall, ROC, Confusion Matrix <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How To: Python - Receiver Operator Characteristic (ROC) Curve and Area Under the Curve\n",
    "\n",
    "import arcpy\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the true class labels and predicted class probabilities into numpy arrays\n",
    "true_labels = arcpy.RasterToNumPyArray(\"true_class_labels.tif\")\n",
    "predicted_probs = arcpy.RasterToNumPyArray(\"predicted_class_probs.tif\")\n",
    "\n",
    "# Flatten the arrays\n",
    "true_labels = true_labels.ravel()\n",
    "predicted_probs = predicted_probs.ravel()\n",
    "\n",
    "# Calculate the false positive rate, true positive rate, and threshold for various thresholds\n",
    "fpr, tpr, thresholds = roc_curve(true_labels, predicted_probs)\n",
    "\n",
    "# Calculate the AUC\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# Plot the ROC curve\n",
    "plt.plot(fpr, tpr, color='darkorange', label='ROC curve (AUC = %0.2f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], color='navy', linestyle='--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### False Positives\n",
    "\n",
    "\n",
    "| Metric | Data Type(s) | Mathematic Definition | ArcPy | Source\n",
    "| :---         |     :---:      |    :---  |  :---: |  :---: |\n",
    "| `False Positives`  | Categorical | `FP` (False Positives): the number of actual negative instances incorrectly predicted as positive | N/A | ChatGPT\n",
    "\n",
    "Similar To: True Positives, ROC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How To: Python - False Positives\n",
    "\n",
    "import arcpy\n",
    "import numpy as np\n",
    "\n",
    "# Load the true class labels and predicted class labels into numpy arrays\n",
    "true_labels = arcpy.RasterToNumPyArray(\"true_class_labels.tif\")\n",
    "predicted_labels = arcpy.RasterToNumPyArray(\"predicted_class_labels.tif\")\n",
    "\n",
    "# Calculate the number of false positives\n",
    "fp = np.sum((true_labels == 0) & (predicted_labels == 1))\n",
    "\n",
    "# Print the number of false positives\n",
    "print(\"False Positives: \", fp)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### True Positives\n",
    "\n",
    "\n",
    "| Metric | Data Type(s) | Mathematic Definition | ArcPy | Source\n",
    "| :---         |     :---:      |    :---  |  :---: |  :---: |\n",
    "| `True Positives`  | Categorical | `TP` (True Positives): the number of correctly predicted positive instances | N/A | ChatGPT\n",
    "\n",
    "Similar To: False Positives, ROC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How To: Python - True Positives\n",
    "\n",
    "import arcpy import numpy as np \n",
    "\n",
    "# Load the true class labels and predicted class labels into numpy arrays \n",
    "true_labels = arcpy.RasterToNumPyArray(\"true_class_labels.tif\")\n",
    "predicted_labels = arcpy.RasterToNumPyArray(\"predicted_class_labels.tif\")\n",
    "\n",
    "# Calculate the number of true positives\n",
    "tp = np.sum((true_labels == 1) & (predicted_labels == 1))\n",
    "\n",
    "# Print the number of true positives\n",
    "print(\"True Positives: \", tp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
